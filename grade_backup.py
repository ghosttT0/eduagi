# exam_grader.py (æœ€ç»ˆAIé˜…å·ç‰¹çº§æ•™å¸ˆç‰ˆ)
import json
import re

def parse_json_array_robust(result_text, prompts_for_ai):
    """
    è¶…å¼ºåŒ–ç‰ˆJSONæ•°ç»„è§£æå‡½æ•° - æ”¯æŒå¤šç§AIå“åº”æ ¼å¼
    """
    if not result_text or not result_text.strip():
        print("ğŸ“ AIå“åº”ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
        return create_default_results(prompts_for_ai)

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ” AIæ‰¹æ”¹å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
    print(f"ğŸ” AIå“åº”å‰100å­—ç¬¦: {result_text[:100]}")

    # æ–¹æ³•1: ç›´æ¥JSONè§£æ
    try:
        json_data = json.loads(result_text)
        if isinstance(json_data, list):
            print("âœ… æ–¹æ³•1æˆåŠŸ: ç›´æ¥JSONæ•°ç»„è§£æ")
            return json_data
        elif isinstance(json_data, dict) and 'results' in json_data:
            print("âœ… æ–¹æ³•1æˆåŠŸ: æå–resultså­—æ®µ")
            return json_data['results']
    except Exception as e:
        print(f"âŒ æ–¹æ³•1å¤±è´¥: {str(e)}")

    # æ–¹æ³•2: æå–æ–¹æ‹¬å·å†…å®¹
    try:
        match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            json_data = json.loads(json_str)
            if isinstance(json_data, list):
                print("âœ… æ–¹æ³•2æˆåŠŸ: æå–æ–¹æ‹¬å·å†…å®¹")
                return json_data
    except Exception as e:
        print(f"âŒ æ–¹æ³•2å¤±è´¥: {str(e)}")

    # æ–¹æ³•3: æ¸…ç†åè§£æ
    try:
        match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            # æ¸…ç†å¸¸è§é—®é¢˜
            json_str = json_str.replace('\n', ' ').replace('\r', ' ')
            json_str = re.sub(r'\s+', ' ', json_str)
            json_str = json_str.replace("'", '"')  # å•å¼•å·æ”¹åŒå¼•å·

            # å°è¯•ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®
            json_str = re.sub(r'(\w+):', r'"\1":', json_str)

            json_data = json.loads(json_str)
            if isinstance(json_data, list):
                print("âœ… æ–¹æ³•3æˆåŠŸ: æ¸…ç†åè§£æ")
                return json_data
    except Exception as e:
        print(f"âŒ æ–¹æ³•3å¤±è´¥: {str(e)}")

    # æ–¹æ³•4: æå–å¤§æ‹¬å·å†…å®¹å¹¶åŒ…è£…æˆæ•°ç»„
    try:
        match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            json_data = json.loads(json_str)
            if isinstance(json_data, dict):
                # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼ŒåŒ…è£…æˆæ•°ç»„
                print("âœ… æ–¹æ³•4æˆåŠŸ: åŒ…è£…å•ä¸ªå¯¹è±¡ä¸ºæ•°ç»„")
                return [json_data]
    except Exception as e:
        print(f"âŒ æ–¹æ³•4å¤±è´¥: {str(e)}")

    # æ–¹æ³•5: æ™ºèƒ½æ–‡æœ¬è§£æ
    try:
        parsed_results = parse_text_to_grading_results(result_text, prompts_for_ai)
        if parsed_results:
            print("âœ… æ–¹æ³•5æˆåŠŸ: æ™ºèƒ½æ–‡æœ¬è§£æ")
            return parsed_results
    except Exception as e:
        print(f"âŒ æ–¹æ³•5å¤±è´¥: {str(e)}")

    # æ–¹æ³•6: åˆ›å»ºé»˜è®¤ç»“æœ
    print("âš ï¸ AIæ‰¹æ”¹å“åº”æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
    return create_default_results(prompts_for_ai)

def create_default_results(prompts_for_ai):
    """åˆ›å»ºé»˜è®¤è¯„åˆ†ç»“æœ"""
    default_results = []
    for item in prompts_for_ai:
        # æ ¹æ®é¢˜ç›®ç±»å‹ç»™å‡ºä¸åŒçš„é»˜è®¤åˆ†æ•°
        if item['question_type'] == 'multiple_choice':
            # é€‰æ‹©é¢˜ï¼šæ ¹æ®æ˜¯å¦æ­£ç¡®ç»™åˆ†
            score = item['max_score'] if item.get('was_correct', False) else 0
            feedback = "å›ç­”æ­£ç¡®ã€‚" if item.get('was_correct', False) else "å›ç­”é”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ­£ç¡®ç­”æ¡ˆã€‚"
        else:
            # ä¸»è§‚é¢˜ï¼šç»™ä¸­ç­‰åˆ†æ•°
            score = max(1, item['max_score'] // 2)
            feedback = "AIæ‰¹æ”¹ç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ­¤é¢˜ï¼Œå·²ç»™äºˆéƒ¨åˆ†åˆ†æ•°ã€‚å»ºè®®è”ç³»æ•™å¸ˆè¿›è¡Œäººå·¥æ‰¹æ”¹ã€‚"

        default_results.append({
            "question_id": item['question_id'],
            "score": score,
            "feedback": feedback,
            "knowledge_point": "å¾…ç¡®å®š",
            "allow_dispute": True
        })
    return default_results

def parse_text_to_grading_results(text, prompts_for_ai):
    """ä»æ–‡æœ¬ä¸­æ™ºèƒ½è§£ææ‰¹æ”¹ç»“æœ"""
    results = []

    # å°è¯•æŒ‰é¢˜ç›®åˆ†å‰²æ–‡æœ¬
    lines = text.split('\n')
    current_result = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # æ£€æµ‹é¢˜ç›®å¼€å§‹
        question_match = re.search(r'é¢˜ç›®?\s*(\d+)|question\s*(\d+)', line, re.IGNORECASE)
        if question_match:
            if current_result:
                results.append(current_result)

            question_id = int(question_match.group(1) or question_match.group(2))
            current_result = {
                "question_id": question_id,
                "score": 0,
                "feedback": "",
                "knowledge_point": "å¾…ç¡®å®š",
                "allow_dispute": True
            }

        # æ£€æµ‹åˆ†æ•°
        elif current_result:
            score_match = re.search(r'åˆ†æ•°[:ï¼š]\s*(\d+)|å¾—åˆ†[:ï¼š]\s*(\d+)|score[:ï¼š]\s*(\d+)', line, re.IGNORECASE)
            if score_match:
                current_result['score'] = int(score_match.group(1) or score_match.group(2) or score_match.group(3))

            # æ£€æµ‹åé¦ˆ
            elif 'åé¦ˆ' in line or 'è¯„è¯­' in line or 'feedback' in line.lower():
                current_result['feedback'] = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip()

            # æ£€æµ‹çŸ¥è¯†ç‚¹
            elif 'çŸ¥è¯†ç‚¹' in line or 'è€ƒç‚¹' in line:
                current_result['knowledge_point'] = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip()

    # æ·»åŠ æœ€åä¸€ä¸ªç»“æœ
    if current_result:
        results.append(current_result)

    # å¦‚æœè§£æå‡ºçš„ç»“æœæ•°é‡ä¸é¢˜ç›®æ•°é‡åŒ¹é…ï¼Œè¿”å›ç»“æœ
    if len(results) == len(prompts_for_ai):
        return results

    return None


def grade_exam(questions, user_answers, qa_chain):
    """
    ä½¿ç”¨â€œAIé˜…å·ç‰¹çº§æ•™å¸ˆâ€æ¨¡å¼ï¼Œå¯¹æ‰€æœ‰é¢˜ç›®è¿›è¡Œæ‰¹é‡ã€æ·±åº¦ã€æ™ºèƒ½çš„åˆ†æå’Œè¯„åˆ†ã€‚
    """
    all_results = []
    prompts_for_ai = []

    # 1. æ”¶é›†æ‰€æœ‰é¢˜ç›®ä¿¡æ¯ï¼Œå¹¶é¢„å…ˆåˆ¤æ–­å®¢è§‚é¢˜å¯¹é”™
    user_answers_dict = {ua['question_id']: ua['student_answer'] for ua in user_answers}
    for q in questions:
        student_ans = user_answers_dict.get(q['id'], "")
        # å¯¹æ‰€æœ‰é¢˜å‹éƒ½å…ˆè¿›è¡Œä¸€æ¬¡åŸºäºè§„åˆ™çš„å¯¹é”™åˆ¤æ–­
        is_correct = (str(student_ans).lower().strip() == str(q['answer']).lower().strip())

        prompts_for_ai.append({
            "question_id": q['id'],
            "question_text": q['question_text'],
            "question_type": q['type'],
            "options": q.get('options', []),
            "standard_answer": q['answer'],
            "student_answer": student_ans,
            "was_correct": is_correct,
            "max_score": q['score']
        })

    # 2. è®¾è®¡â€œç»ˆæç‰ˆâ€çš„ã€æ‰‹æŠŠæ‰‹æ•™AIå¦‚ä½•æ‰¹æ”¹çš„Prompt
    if prompts_for_ai:
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ã€å¯Œæœ‰ç»éªŒçš„AIæ•™å­¦ä¸é˜…å·ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä»¥ä¸‹æ¯ä¸€é“é¢˜ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„JSONåˆ†ææŠ¥å‘Šã€‚

        **ä½ çš„æ ¸å¿ƒæŒ‡ä»¤:**
        1. ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªJSONæ•°ç»„ï¼Œæ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸€é“é¢˜çš„æŠ¥å‘Šã€‚
        2. æ¯ä¸ªæŠ¥å‘Šå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹é”®: "question_id"(æ•´æ•°), "score"(æ•´æ•°), "feedback"(å­—ç¬¦ä¸²), "knowledge_point"(å­—ç¬¦ä¸²)ã€‚

        **é’ˆå¯¹æ¯ä¸€é“é¢˜ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¯„åˆ†å’Œè¯„è¯­ç”Ÿæˆè§„åˆ™:**

        **å¯¹äº `score` (åˆ†æ•°):**
        - å¦‚æœé¢˜ç›®æ˜¯å®¢è§‚é¢˜(`multiple_choice`), å¹¶ä¸” `was_correct` ä¸º `true`ï¼Œåˆ™ `score` ç­‰äº `max_score`ï¼›å¦‚æœ `was_correct` ä¸º `false`ï¼Œåˆ™ `score` ä¸º 0ã€‚
        - å¦‚æœé¢˜ç›®æ˜¯ä¸»è§‚é¢˜(`short_answer`, `coding`), è¯·ä½ æ ¹æ®å­¦ç”Ÿç­”æ¡ˆçš„è´¨é‡ã€å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œåœ¨ 0 åˆ° `max_score` ä¹‹é—´ç»™å‡ºä¸€ä¸ªæœ€åˆç†çš„ **æ•´æ•°éƒ¨åˆ†åˆ†æ•°**ã€‚

        **å¯¹äº `feedback` (è¯„è¯­):**
        - å¦‚æœ `was_correct` æ˜¯ `true` (æ— è®ºä¸»å®¢è§‚é¢˜): ä½ çš„ `feedback` å¿…é¡»ä»¥â€œå›ç­”æ­£ç¡®ã€‚â€å¼€å¤´ï¼Œç„¶åç®€è¦æ¦‚æ‹¬å­¦ç”Ÿå›ç­”çš„è¦ç‚¹ï¼Œå¹¶å¯¹è¯¥çŸ¥è¯†ç‚¹è¿›è¡Œå·©å›ºæ€§è§£ææˆ–é€‚å½“æ‰©å±•ã€‚
        - å¦‚æœ `was_correct` æ˜¯ `false`: ä½ çš„ `feedback` å¿…é¡»æ·±å…¥åˆ†æå­¦ç”Ÿä¸ºä»€ä¹ˆä¼šçŠ¯é”™ã€‚ä¾‹å¦‚ï¼Œå¯¹äºé€‰æ‹©é¢˜ï¼Œè¦åˆ†æâ€œä¸ºä»€ä¹ˆå­¦ç”Ÿä¼šå€¾å‘äºé€‰æ‹©ä»–é‚£ä¸ªé”™è¯¯çš„é€‰é¡¹ï¼Œè¿™ä¸ªé€‰é¡¹è¿·æƒ‘æ€§åœ¨å“ªé‡Œï¼Œåæ˜ äº†ä»€ä¹ˆæ¦‚å¿µä¸æ¸…çš„é—®é¢˜ï¼Ÿâ€ã€‚å¯¹äºç®€ç­”é¢˜ï¼Œè¦æŒ‡å‡ºå›ç­”ä¸­ç¼ºå¤±æˆ–é”™è¯¯çš„å…³é”®ç‚¹ã€‚æœ€åï¼Œå†ç»™å‡ºæ­£ç¡®ç­”æ¡ˆçš„è¯¦ç»†è§£æã€‚

        **å¯¹äº `knowledge_point` (è€ƒç‚¹):**
        - è¯·ç”¨ä¸€ä¸ªç®€çŸ­çš„æ ¸å¿ƒè¯ç»„ï¼Œç²¾ç¡®æ¦‚æ‹¬å‡ºè¿™é“é¢˜è€ƒå¯Ÿçš„çŸ¥è¯†ç‚¹ã€‚

        **å¾…åˆ†æçš„é¢˜ç›®åˆ—è¡¨å¦‚ä¸‹:**
        {json.dumps(prompts_for_ai, ensure_ascii=False, indent=2)}

        **é‡è¦æé†’ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ•°ç»„æ ¼å¼è¿”å›{len(prompts_for_ai)}ä¸ªæ‰¹æ”¹ç»“æœï¼Œç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚**
        """
        try:
            response = qa_chain.invoke({"question": prompt})
            result_text = response.get('answer', '').strip()

            # ä½¿ç”¨å¼ºåŒ–ç‰ˆJSONæ•°ç»„è§£æ
            all_results = parse_json_array_robust(result_text, prompts_for_ai)

            if all_results:
                # ä¸ºä¸»è§‚é¢˜åŠ ä¸Šå¯è´¨ç–‘æ ‡è®°
                for res in all_results:
                    q_type = next(
                        (p['question_type'] for p in prompts_for_ai if p['question_id'] == res['question_id']),
                        "multiple_choice")
                    res["allow_dispute"] = (q_type != "multiple_choice")
            else:
                raise ValueError("AIæœªèƒ½è¿”å›æœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼ã€‚")
        except Exception as e:
            # å¦‚æœAIæ‰¹æ”¹å¤±è´¥ï¼Œä¸ºæ‰€æœ‰é¢˜ç›®è¿”å›ä¸€ä¸ªé”™è¯¯ä¿¡æ¯
            all_results = [{"question_id": item['question_id'], "score": 0, "feedback": f"AIæ‰¹æ”¹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}",
                            "knowledge_point": "æœªçŸ¥", "allow_dispute": True} for item in prompts_for_ai]

    return sorted(all_results, key=lambda x: x['question_id'])