# views/visualization_view.py (å¢åŠ D3å›¾è°±ç”Ÿæˆè¿‡ç¨‹å±•ç¤º)
import json
import re
import uuid
from streamlit.components.v1 import html
from jinja2 import Template
from utils import load_conversational_chain
import streamlit as st
import stylecloud
import jieba
from database import SessionLocal, ChatHistory, KnowledgeMastery
import os
from PIL import Image
import numpy as np

def parse_d3_graph_json(result_text, keyword):
    """
    å¼ºåŒ–ç‰ˆD3å›¾è°±JSONè§£æå‡½æ•°
    """
    if not result_text or not result_text.strip():
        return create_default_graph(keyword)

    # æ–¹æ³•1: ç›´æ¥JSONè§£æ
    try:
        json_data = json.loads(result_text)
        if isinstance(json_data, dict) and 'name' in json_data:
            return json_data
    except:
        pass

    # æ–¹æ³•2: æå–å¤§æ‹¬å·å†…å®¹
    try:
        match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            json_data = json.loads(json_str)
            if isinstance(json_data, dict) and 'name' in json_data:
                return json_data
    except:
        pass

    # æ–¹æ³•3: æ¸…ç†åè§£æ
    try:
        match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            # æ¸…ç†å¸¸è§é—®é¢˜
            json_str = json_str.replace('\n', ' ').replace('\r', ' ')
            json_str = re.sub(r'\s+', ' ', json_str)
            json_str = json_str.replace("'", '"')  # å•å¼•å·æ”¹åŒå¼•å·

            # å°è¯•ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®
            json_str = re.sub(r'(\w+):', r'"\1":', json_str)

            json_data = json.loads(json_str)
            if isinstance(json_data, dict) and 'name' in json_data:
                return json_data
    except:
        pass

    # æ–¹æ³•4: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
    try:
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„JSONå¯¹è±¡
        brace_count = 0
        start_pos = -1

        for i, char in enumerate(result_text):
            if char == '{':
                if start_pos == -1:
                    start_pos = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_pos != -1:
                    # æ‰¾åˆ°å®Œæ•´çš„JSONå¯¹è±¡
                    json_str = result_text[start_pos:i+1]
                    json_data = json.loads(json_str)
                    if isinstance(json_data, dict) and 'name' in json_data:
                        return json_data
                    start_pos = -1
    except:
        pass

    # æ–¹æ³•5: è¿”å›é»˜è®¤å›¾è°±
    return create_default_graph(keyword)

def create_default_graph(keyword):
    """åˆ›å»ºé»˜è®¤çš„D3å›¾è°±ç»“æ„"""
    return {
        "name": keyword,
        "children": [
            {
                "name": "åŸºæœ¬æ¦‚å¿µ",
                "children": [
                    {"name": "å®šä¹‰ä¸ç‰¹ç‚¹"},
                    {"name": "å‘å±•å†å²"},
                    {"name": "åº”ç”¨é¢†åŸŸ"}
                ]
            },
            {
                "name": "æ ¸å¿ƒåŸç†",
                "children": [
                    {"name": "å·¥ä½œæœºåˆ¶"},
                    {"name": "ç®—æ³•æµç¨‹"},
                    {"name": "æŠ€æœ¯è¦ç‚¹"}
                ]
            },
            {
                "name": "å®é™…åº”ç”¨",
                "children": [
                    {"name": "å…¸å‹æ¡ˆä¾‹"},
                    {"name": "å®ç°æ–¹æ³•"},
                    {"name": "å‘å±•è¶‹åŠ¿"}
                ]
            }
        ]
    }

def add_generation_id(node, gen_id):
    """é€’å½’å‡½æ•°ï¼Œä¸ºå›¾è°±ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡ID"""
    node['generation_id'] = gen_id
    if 'children' in node:
        for child in node['children']:
            add_generation_id(child, gen_id)

def add_mastery_info(node, mastery_dict):
    """é€’å½’å‡½æ•°ï¼Œä¸ºå›¾è°±ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æ·»åŠ æŒæ¡ç¨‹åº¦ä¿¡æ¯"""
    knowledge_point = node['name']
    if knowledge_point in mastery_dict:
        mastery_level = mastery_dict[knowledge_point]
        node['mastery_level'] = mastery_level
        # æ ¹æ®æŒæ¡ç¨‹åº¦è®¾ç½®é¢œè‰²å’Œå¤§å°
        if mastery_level == 1:  # è–„å¼±ç¯èŠ‚
            node['color'] = '#ff4444'  # çº¢è‰²
            node['size'] = 12
        elif mastery_level == 2:  # åŸºæœ¬æŒæ¡
            node['color'] = '#ffaa00'  # é»„è‰²
            node['size'] = 15
        else:  # ç†Ÿç»ƒæŒæ¡
            node['color'] = '#44ff44'  # ç»¿è‰²
            node['size'] = 18
    else:
        # æœªè¯„ä¼°çš„çŸ¥è¯†ç‚¹ä½¿ç”¨é»˜è®¤é¢œè‰²
        node['mastery_level'] = 0
        node['color'] = '#888888'  # ç°è‰²
        node['size'] = 15

    if 'children' in node:
        for child in node['children']:
            add_mastery_info(child, mastery_dict)

def render():
    """æ¸²æŸ“åŒ…å«è¯äº‘å’ŒD3çŸ¥è¯†å›¾è°±çš„æ•°æ®å¯è§†åŒ–é¡µé¢"""
    st.title("ğŸ“Š æ•°æ®å¯è§†åŒ–ä¸­å¿ƒ")

    tab1, tab2 = st.tabs(["**â˜ï¸ çŸ¥è¯†ç‚¹è¯äº‘**", "**ğŸ§  AIçŸ¥è¯†å›¾è°± (D3.js)**"])

    # --- Tab 1: çŸ¥è¯†ç‚¹è¯äº‘ (ä½¿ç”¨æ‚¨çš„ç¯æ³¡å›¾ç‰‡) ---
    with tab1:
        st.info("AIé€šè¿‡åˆ†ææ‰€æœ‰å­¦ç”Ÿçš„æé—®ï¼Œæ™ºèƒ½æå–å‡ºå½“å‰æœ€å—å…³æ³¨çš„çŸ¥è¯†ç„¦ç‚¹ã€‚")

        # é¢„è®¾AI/æœºå™¨å­¦ä¹ å…³é”®è¯
        preset_keywords = {
            "å¾ªç¯ç¥ç»ç½‘ç»œ": 25, "å·ç§¯ç¥ç»ç½‘ç»œ": 23, "æ·±åº¦å­¦ä¹ ": 20, "æœºå™¨å­¦ä¹ ": 18,
            "äººå·¥æ™ºèƒ½": 16, "ç¥ç»ç½‘ç»œ": 15, "è‡ªç„¶è¯­è¨€å¤„ç†": 14, "è®¡ç®—æœºè§†è§‰": 13,
            "æ•°æ®æŒ–æ˜": 12, "ç®—æ³•ä¼˜åŒ–": 11, "Pythonç¼–ç¨‹": 10, "æ•°æ®ç»“æ„": 9,
            "çº¿æ€§ä»£æ•°": 8, "æ¦‚ç‡ç»Ÿè®¡": 7, "åå‘ä¼ æ’­": 6, "æ¢¯åº¦ä¸‹é™": 5,
            "ç‰¹å¾æå–": 4, "æ¨¡å‹è®­ç»ƒ": 3, "è¿‡æ‹Ÿåˆ": 2, "æ­£åˆ™åŒ–": 1
        }

        # ç”Ÿæˆè¯äº‘æŒ‰é’®
        if st.button("ğŸ¯ ç”ŸæˆçŸ¥è¯†ç‚¹è¯äº‘", key="generate_wordcloud", use_container_width=True):

            with st.spinner("æ­£åœ¨ç”ŸæˆçŸ¥è¯†ç‚¹è¯äº‘..."):
                try:
                    # è·å–å­¦ç”Ÿæé—®æ•°æ®
                    db = SessionLocal()
                    all_questions = db.query(ChatHistory.message).filter(ChatHistory.is_user == True).all()

                    # åˆå¹¶é¢„è®¾å…³é”®è¯å’Œå­¦ç”Ÿæé—®
                    combined_keywords = preset_keywords.copy()

                    if all_questions:
                        # åˆ†æå­¦ç”Ÿæé—®
                        text = " ".join([q[0] for q in all_questions])
                        word_list = jieba.cut(text, cut_all=False)

                        # è¿‡æ»¤åœç”¨è¯
                        filtered_words = [word for word in word_list
                                        if len(word) >= 2 and word not in ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å¯ä»¥', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¯·é—®', 'è€å¸ˆ', 'åŒå­¦']]

                        # ç»Ÿè®¡è¯é¢‘
                        from collections import Counter
                        word_freq = Counter(filtered_words)

                        # åˆå¹¶åˆ°é¢„è®¾å…³é”®è¯ä¸­
                        for word, freq in word_freq.items():
                            if word in combined_keywords:
                                combined_keywords[word] += freq
                            else:
                                combined_keywords[word] = freq

                    db.close()

                    # ä½¿ç”¨WordCloudç”Ÿæˆç¯æ³¡å½¢çŠ¶è¯äº‘
                    from wordcloud import WordCloud
                    import matplotlib.pyplot as plt

                    # åŠ è½½æ­£ç¡®çš„ç¯æ³¡è’™ç‰ˆï¼ˆæ–‡å­—å¡«å……åœ¨ç¯æ³¡å†…éƒ¨ï¼‰
                    try:
                        # ä¼˜å…ˆå°è¯•åŠ è½½æ­£ç¡®çš„ç¯æ³¡è’™ç‰ˆ
                        mask_files = [
                            "correct_lightbulb_mask.png",      # æ ‡å‡†ç‰ˆ
                            "detailed_lightbulb_mask.png",     # è¯¦ç»†ç‰ˆ
                            "simple_filled_lightbulb.png"      # ç®€å•ç‰ˆ
                        ]

                        mask_array = None
                        mask_used = None

                        for mask_file in mask_files:
                            try:
                                mask_image = Image.open(mask_file)
                                mask_array = np.array(mask_image)

                                # è½¬æ¢ä¸ºç°åº¦
                                if len(mask_array.shape) == 3:
                                    mask_array = np.mean(mask_array, axis=2)

                                # ç¡®ä¿è’™ç‰ˆæ ¼å¼æ­£ç¡®
                                mask_array = mask_array.astype(np.uint8)
                                mask_used = mask_file
                                break
                            except:
                                continue

                    except Exception as e:
                        st.warning(f"æ— æ³•åŠ è½½ç¯æ³¡è’™ç‰ˆï¼Œä½¿ç”¨é»˜è®¤å½¢çŠ¶: {e}")
                        mask_array = None

                    # ç”Ÿæˆè¯äº‘
                    # å°è¯•æ‰¾åˆ°æ­£ç¡®çš„å­—ä½“è·¯å¾„
                    font_paths = ['SimHei.ttf', 'simhei.ttf', 'C:/Windows/Fonts/simhei.ttf']
                    font_path = None
                    for fp in font_paths:
                        if os.path.exists(fp):
                            font_path = fp
                            break

                    wordcloud_params = {
                        'font_path': font_path,
                        'width': 800,
                        'height': 800,
                        'background_color': 'white',
                        'max_words': 60,  # å¢åŠ è¯æ±‡æ•°é‡
                        'colormap': 'viridis',  # ä½¿ç”¨ç§‘æŠ€æ„Ÿé…è‰²
                        'relative_scaling': 0.5,  # è°ƒæ•´ç›¸å¯¹ç¼©æ”¾
                        'random_state': 42,
                        'collocations': False,  # é¿å…é‡å¤è¯ç»„
                        'prefer_horizontal': 0.6,  # å¹³è¡¡æ°´å¹³å’Œå‚ç›´æ–‡å­—
                        'min_font_size': 8,  # è®¾ç½®æœ€å°å­—ä½“
                        'max_font_size': 100,  # è®¾ç½®æœ€å¤§å­—ä½“
                        'scale': 2  # æé«˜æ¸…æ™°åº¦
                    }

                    # å¦‚æœæœ‰è’™ç‰ˆï¼Œæ·»åŠ è’™ç‰ˆå‚æ•°
                    if mask_array is not None:
                        wordcloud_params['mask'] = mask_array
                        wordcloud_params['contour_width'] = 1
                        wordcloud_params['contour_color'] = '#FFD700'  # é‡‘è‰²è½®å»“
                        # æ ¹æ®è’™ç‰ˆè°ƒæ•´å°ºå¯¸
                        wordcloud_params['width'] = mask_array.shape[1]
                        wordcloud_params['height'] = mask_array.shape[0]

                    wordcloud = WordCloud(**wordcloud_params).generate_from_frequencies(combined_keywords)

                    # è½¬æ¢ä¸ºå›¾ç‰‡å¹¶æ˜¾ç¤º
                    fig, ax = plt.subplots(figsize=(10, 10))  # æ­£æ–¹å½¢ç”»å¸ƒé€‚åº”ç¯æ³¡å½¢çŠ¶
                    ax.imshow(wordcloud, interpolation='bilinear')
                    ax.axis('off')

                    # è®¾ç½®ç´§å‡‘å¸ƒå±€
                    plt.tight_layout()

                    st.success("ğŸ’¡ ç¯æ³¡å½¢çŠ¶çŸ¥è¯†ç‚¹è¯äº‘ç”Ÿæˆå®Œæˆï¼")
                    st.pyplot(fig)
                    plt.close()

                    # æ·»åŠ è¯´æ˜
                    st.markdown("ğŸ’¡ **åˆ›æ„è¯´æ˜**: è¯äº‘é‡‡ç”¨ç¯æ³¡å½¢çŠ¶ï¼Œè±¡å¾ç€çŸ¥è¯†çš„å¯å‘å’Œåˆ›æ–°æ€ç»´ï¼")

                    # æ˜¾ç¤ºå…³é”®è¯ç»Ÿè®¡
                    with st.expander("ğŸ“Š è¯äº‘ç»Ÿè®¡ä¿¡æ¯", expanded=False):
                        st.markdown(f"**æ€»å…³é”®è¯æ•°**: {len(combined_keywords)}")
                        st.markdown(f"**å­¦ç”Ÿæé—®æ•°**: {len(all_questions) if all_questions else 0}")

                        # æ˜¾ç¤ºçƒ­é—¨å…³é”®è¯
                        top_keywords = sorted(combined_keywords.items(), key=lambda x: x[1], reverse=True)[:10]
                        st.markdown("##### ğŸ”¥ çƒ­é—¨å…³é”®è¯ TOP 10")
                        for i, (word, freq) in enumerate(top_keywords, 1):
                            st.markdown(f"{i}. **{word}** - æƒé‡ {freq}")

                except Exception as e:
                    st.error(f"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™: {e}")
                    # æ˜¾ç¤ºé¢„è®¾å…³é”®è¯ä½œä¸ºå¤‡é€‰
                    st.markdown("##### ğŸ“‹ é¢„è®¾AI/æœºå™¨å­¦ä¹ å…³é”®è¯")
                    cols = st.columns(4)
                    keywords_list = list(preset_keywords.keys())
                    for i, keyword in enumerate(keywords_list):
                        with cols[i % 4]:
                            st.markdown(f"â€¢ **{keyword}**")



        # --- Tab 2: D3çŸ¥è¯†å›¾è°± (å·²ä¿®å¤) ---
    with tab2:
        """æ¸²æŸ“åŒ…å«D3å¾„å‘æ ‘çŠ¶å›¾çš„æ•°æ®å¯è§†åŒ–é¡µé¢"""
        st.title("ğŸ“Š æ•°æ®å¯è§†åŒ–ä¸­å¿ƒ")

        # æˆ‘ä»¬æš‚æ—¶ä¸“æ³¨äºD3å›¾è°±ï¼Œæ‚¨å¯ä»¥ä¹‹åæŠŠè¯äº‘çš„TabåŠ å›æ¥
        st.info("è¯·è¾“å…¥ä¸€ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼ŒAIå°†å›´ç»•å®ƒç”Ÿæˆä¸€ä¸ªå±‚çº§æ¸…æ™°ã€ä»ä¸­å¿ƒå‘æ•£çš„çŸ¥è¯†å›¾è°±ã€‚")

        qa_chain = load_conversational_chain()

        # åˆå§‹åŒ–ç”¨äºå­˜å‚¨æ‰€æœ‰å›¾è°±çš„ä¼šè¯çŠ¶æ€
        if "d3_constellation" not in st.session_state:
            st.session_state.d3_constellation = None

        # --- è¾“å…¥ä¸ç”Ÿæˆè¡¨å• ---
        col1, col2 = st.columns([2, 1])
        with col1:
            keyword = st.text_input("è¯·è¾“å…¥æ–°çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š", placeholder="ä¾‹å¦‚ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ")
        with col2:
            # å…è®¸ç”¨æˆ·é€‰æ‹©ä¸€ä¸ªå·²æœ‰çš„å›¾è°±ä½œä¸ºè¿æ¥ç‚¹
            if st.session_state.d3_constellation:
                # é€’å½’å‡½æ•°ï¼Œç”¨äºè·å–æ‰€æœ‰èŠ‚ç‚¹çš„åç§°
                def get_all_node_names(node, names):
                    names.append(node['name'])
                    if 'children' in node:
                        for child in node['children']:
                            get_all_node_names(child, names)
                    return names

                existing_nodes = get_all_node_names(st.session_state.d3_constellation, [])
                connection_point = st.selectbox(
                    "å¯é€‰ï¼šå°†æ–°å›¾è°±è¿æ¥åˆ°å·²æœ‰èŠ‚ç‚¹ä¸Š",
                    options=["ï¼ˆä¸è¿æ¥ï¼Œä½œä¸ºæ–°çš„ä¸­å¿ƒï¼‰"] + existing_nodes
                )
            else:
                connection_point = "ï¼ˆä¸è¿æ¥ï¼Œä½œä¸ºæ–°çš„ä¸­å¿ƒï¼‰"

        if st.button("ç”Ÿæˆå¹¶è¿æ¥D3çŸ¥è¯†å›¾è°±", use_container_width=True):
            if keyword:
                with st.spinner(f"AIæ­£åœ¨å›´ç»•â€œ{keyword}â€æ„å»ºçŸ¥è¯†èŠ‚ç‚¹..."):
                    try:
                        # è®¾è®¡Promptï¼Œè®©AIç”ŸæˆD3æ ‘çŠ¶å›¾éœ€è¦çš„å±‚çº§JSON
                        prompt = f"""
                            ä½ æ˜¯ä¸€ä½çŸ¥è¯†ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·ä¸ºä¸»é¢˜ â€œ{keyword}â€ åˆ›å»ºä¸€ä¸ªç¬¦åˆD3å±‚çº§å¸ƒå±€çš„JSONå¯¹è±¡ã€‚
                            JSONå¯¹è±¡å¿…é¡»æœ‰ "name" é”®ä½œä¸ºæ ¹èŠ‚ç‚¹åç§°ï¼Œä»¥åŠä¸€ä¸ª "children" æ•°ç»„å­˜æ”¾æ‰€æœ‰å­èŠ‚ç‚¹ã€‚
                            """
                        response = qa_chain.invoke({"question": prompt, "chat_history": []})
                        result_text = response['answer'].strip()

                        # ä½¿ç”¨å¼ºåŒ–ç‰ˆJSONè§£æ
                        new_graph_data = parse_d3_graph_json(result_text, keyword)

                        if new_graph_data:

                            if connection_point != "ï¼ˆä¸è¿æ¥ï¼Œä½œä¸ºæ–°çš„ä¸­å¿ƒï¼‰":
                                # é€’å½’å‡½æ•°ï¼Œç”¨äºåœ¨æ˜Ÿåº§ä¸­æŸ¥æ‰¾å¹¶æ’å…¥æ–°èŠ‚ç‚¹
                                def find_and_insert(node, parent_name, new_child):
                                    if node['name'] == parent_name:
                                        if 'children' not in node:
                                            node['children'] = []
                                        node['children'].append(new_child)
                                        return True
                                    if 'children' in node:
                                        for child in node['children']:
                                            if find_and_insert(child, parent_name, new_child):
                                                return True
                                    return False

                                find_and_insert(st.session_state.d3_constellation, connection_point, new_graph_data)
                            else:
                                # å¦‚æœä¸è¿æ¥ï¼Œå°±å°†æ–°å›¾è°±ä½œä¸ºç¬¬ä¸€ä¸ªæˆ–æ–°çš„ä¸­å¿ƒ
                                st.session_state.d3_constellation = new_graph_data

                            st.success("çŸ¥è¯†å›¾è°±å·²æˆåŠŸç”Ÿæˆ/è¿æ¥ï¼")
                        else:
                            st.warning("AIè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å›¾è°±æ¨¡æ¿")
                            # ä½¿ç”¨é»˜è®¤å›¾è°±
                            default_graph = create_default_graph(keyword)
                            if connection_point != "ï¼ˆä¸è¿æ¥ï¼Œä½œä¸ºæ–°çš„ä¸­å¿ƒï¼‰":
                                find_and_insert(st.session_state.d3_constellation, connection_point, default_graph)
                            else:
                                st.session_state.d3_constellation = default_graph
                            st.success("å·²ä½¿ç”¨é»˜è®¤æ¨¡æ¿ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼")
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå›¾è°±æ—¶å‡ºé”™: {e}")
                        # æ˜¾ç¤ºAIåŸå§‹è¿”å›å†…å®¹ç”¨äºè°ƒè¯•
                        with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯", expanded=False):
                            st.code(result_text if 'result_text' in locals() else "æ— è¿”å›å†…å®¹", language="text")
            else:
                st.warning("è¯·è¾“å…¥å…³é”®è¯ï¼")

        # --- æ¸²æŸ“çŸ¥è¯†æ˜Ÿåº§å›¾ ---
        if st.session_state.d3_constellation:
            st.markdown("---")
            st.subheader("å¯äº¤äº’çš„D3çŸ¥è¯†æ˜Ÿåº§å›¾")

            # æ·»åŠ æŒæ¡ç¨‹åº¦å¯è§†åŒ–é€‰é¡¹ï¼ˆä»…å¯¹å­¦ç”Ÿæ˜¾ç¤ºï¼‰
            show_mastery = False
            current_user_role = st.session_state.get("role", "")
            if current_user_role == "å­¦ç”Ÿ":
                show_mastery = st.checkbox("æ˜¾ç¤ºçŸ¥è¯†æŒæ¡ç¨‹åº¦", value=False, help="æ ¹æ®æ‚¨çš„è‡ªæˆ‘è¯„ä¼°æ˜¾ç¤ºä¸åŒé¢œè‰²")

            # å‡†å¤‡å›¾è°±æ•°æ®
            graph_data = st.session_state.d3_constellation.copy()

            if show_mastery and current_user_role == "å­¦ç”Ÿ":
                # è·å–å½“å‰ç”¨æˆ·çš„çŸ¥è¯†æŒæ¡ç¨‹åº¦æ•°æ®
                current_user_id = st.session_state.get("user_id")
                if current_user_id:
                    db = SessionLocal()
                    try:
                        mastery_records = db.query(KnowledgeMastery).filter(
                            KnowledgeMastery.student_id == current_user_id
                        ).all()

                        # åˆ›å»ºæŒæ¡ç¨‹åº¦å­—å…¸
                        mastery_dict = {record.knowledge_point: record.mastery_level for record in mastery_records}

                        # ä¸ºå›¾è°±èŠ‚ç‚¹æ·»åŠ æŒæ¡ç¨‹åº¦ä¿¡æ¯
                        add_mastery_info(graph_data, mastery_dict)

                        # æ˜¾ç¤ºå›¾ä¾‹
                        st.markdown("**å›¾ä¾‹ï¼š** ğŸ”´ è–„å¼±ç¯èŠ‚ | ğŸŸ¡ åŸºæœ¬æŒæ¡ | ğŸŸ¢ ç†Ÿç»ƒæŒæ¡ | âšª æœªè¯„ä¼°")

                    finally:
                        db.close()

            template_path = os.path.join("templates", "d3_graph.html")
            with open(template_path, "r", encoding="utf-8") as f:
                template = Template(f.read())

            graph_json = json.dumps(graph_data)
            unique_id = str(uuid.uuid4())
            html_content = template.render(
                graph_data=graph_json,
                unique_id=unique_id,
                show_mastery=show_mastery
            )

            html(html_content, height=800, scrolling=False)