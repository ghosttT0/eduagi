# exam_grader.py (æœ€ç»ˆAIé˜…å·ç‰¹çº§æ•™å¸ˆç‰ˆ)
import json
import re

def parse_json_array_robust(result_text, prompts_for_ai):
    """
    è¶…å¼ºåŒ–ç‰ˆJSONæ•°ç»„è§£æå‡½æ•° - æ”¯æŒå¤šç§AIå“åº”æ ¼å¼
    """
    if not result_text or not result_text.strip():
        print("ğŸ“ AIå“åº”ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
        return create_default_results(prompts_for_ai)

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ” AIæ‰¹æ”¹å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
    print(f"ğŸ” AIå“åº”å‰100å­—ç¬¦: {result_text[:100]}")

    # æ–¹æ³•1: ç›´æ¥JSONè§£æ
    try:
        json_data = json.loads(result_text)
        if isinstance(json_data, list):
            print("âœ… æ–¹æ³•1æˆåŠŸ: ç›´æ¥JSONæ•°ç»„è§£æ")
            return json_data
        elif isinstance(json_data, dict) and 'results' in json_data:
            print("âœ… æ–¹æ³•1æˆåŠŸ: æå–resultså­—æ®µ")
            return json_data['results']
    except Exception as e:
        print(f"âŒ æ–¹æ³•1å¤±è´¥: {str(e)}")

    # æ–¹æ³•2: æå–æ–¹æ‹¬å·å†…å®¹
    try:
        match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            json_data = json.loads(json_str)
            if isinstance(json_data, list):
                print("âœ… æ–¹æ³•2æˆåŠŸ: æå–æ–¹æ‹¬å·å†…å®¹")
                return json_data
    except Exception as e:
        print(f"âŒ æ–¹æ³•2å¤±è´¥: {str(e)}")

    # æ–¹æ³•3: æ¸…ç†åè§£æ
    try:
        match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            # æ¸…ç†å¸¸è§é—®é¢˜
            json_str = json_str.replace('\n', ' ').replace('\r', ' ')
            json_str = re.sub(r'\s+', ' ', json_str)
            json_str = json_str.replace("'", '"')  # å•å¼•å·æ”¹åŒå¼•å·

            # å°è¯•ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®
            json_str = re.sub(r'(\w+):', r'"\1":', json_str)

            json_data = json.loads(json_str)
            if isinstance(json_data, list):
                print("âœ… æ–¹æ³•3æˆåŠŸ: æ¸…ç†åè§£æ")
                return json_data
    except Exception as e:
        print(f"âŒ æ–¹æ³•3å¤±è´¥: {str(e)}")

    # æ–¹æ³•4: æå–å¤§æ‹¬å·å†…å®¹å¹¶åŒ…è£…æˆæ•°ç»„
    try:
        match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            json_data = json.loads(json_str)
            if isinstance(json_data, dict):
                # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼ŒåŒ…è£…æˆæ•°ç»„
                print("âœ… æ–¹æ³•4æˆåŠŸ: åŒ…è£…å•ä¸ªå¯¹è±¡ä¸ºæ•°ç»„")
                return [json_data]
    except Exception as e:
        print(f"âŒ æ–¹æ³•4å¤±è´¥: {str(e)}")

    # æ–¹æ³•5: æ™ºèƒ½æ–‡æœ¬è§£æ
    try:
        parsed_results = parse_text_to_grading_results(result_text, prompts_for_ai)
        if parsed_results:
            print("âœ… æ–¹æ³•5æˆåŠŸ: æ™ºèƒ½æ–‡æœ¬è§£æ")
            return parsed_results
    except Exception as e:
        print(f"âŒ æ–¹æ³•5å¤±è´¥: {str(e)}")

    # æ–¹æ³•6: åˆ›å»ºé»˜è®¤ç»“æœ
    print("âš ï¸ AIæ‰¹æ”¹å“åº”æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
    return create_default_results(prompts_for_ai)

def create_default_results(prompts_for_ai):
    """åˆ›å»ºé»˜è®¤è¯„åˆ†ç»“æœ"""
    default_results = []
    for item in prompts_for_ai:
        # æ ¹æ®é¢˜ç›®ç±»å‹ç»™å‡ºä¸åŒçš„é»˜è®¤åˆ†æ•°
        if item['question_type'] == 'multiple_choice':
            # é€‰æ‹©é¢˜ï¼šæ ¹æ®æ˜¯å¦æ­£ç¡®ç»™åˆ†
            score = item['max_score'] if item.get('was_correct', False) else 0
            feedback = "å›ç­”æ­£ç¡®ã€‚" if item.get('was_correct', False) else "å›ç­”é”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ­£ç¡®ç­”æ¡ˆã€‚"
        else:
            # ä¸»è§‚é¢˜ï¼šç»™ä¸­ç­‰åˆ†æ•°
            score = max(1, item['max_score'] // 2)
            feedback = "AIæ‰¹æ”¹ç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ­¤é¢˜ï¼Œå·²ç»™äºˆéƒ¨åˆ†åˆ†æ•°ã€‚å»ºè®®è”ç³»æ•™å¸ˆè¿›è¡Œäººå·¥æ‰¹æ”¹ã€‚"
        
        default_results.append({
            "question_id": item['question_id'],
            "score": score,
            "feedback": feedback,
            "knowledge_point": "å¾…ç¡®å®š",
            "allow_dispute": True
        })
    return default_results

def parse_text_to_grading_results(text, prompts_for_ai):
    """ä»æ–‡æœ¬ä¸­æ™ºèƒ½è§£ææ‰¹æ”¹ç»“æœ"""
    results = []
    
    # å°è¯•æŒ‰é¢˜ç›®åˆ†å‰²æ–‡æœ¬
    lines = text.split('\n')
    current_result = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # æ£€æµ‹é¢˜ç›®å¼€å§‹
        question_match = re.search(r'é¢˜ç›®?\s*(\d+)|question\s*(\d+)', line, re.IGNORECASE)
        if question_match:
            if current_result:
                results.append(current_result)
            
            question_id = int(question_match.group(1) or question_match.group(2))
            current_result = {
                "question_id": question_id,
                "score": 0,
                "feedback": "",
                "knowledge_point": "å¾…ç¡®å®š",
                "allow_dispute": True
            }
        
        # æ£€æµ‹åˆ†æ•°
        elif current_result:
            score_match = re.search(r'åˆ†æ•°[:ï¼š]\s*(\d+)|å¾—åˆ†[:ï¼š]\s*(\d+)|score[:ï¼š]\s*(\d+)', line, re.IGNORECASE)
            if score_match:
                current_result['score'] = int(score_match.group(1) or score_match.group(2) or score_match.group(3))
            
            # æ£€æµ‹åé¦ˆ
            elif 'åé¦ˆ' in line or 'è¯„è¯­' in line or 'feedback' in line.lower():
                current_result['feedback'] = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip()
            
            # æ£€æµ‹çŸ¥è¯†ç‚¹
            elif 'çŸ¥è¯†ç‚¹' in line or 'è€ƒç‚¹' in line:
                current_result['knowledge_point'] = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip()
    
    # æ·»åŠ æœ€åä¸€ä¸ªç»“æœ
    if current_result:
        results.append(current_result)
    
    # å¦‚æœè§£æå‡ºçš„ç»“æœæ•°é‡ä¸é¢˜ç›®æ•°é‡åŒ¹é…ï¼Œè¿”å›ç»“æœ
    if len(results) == len(prompts_for_ai):
        return results
    
    return None


def grade_exam(questions, user_answers, qa_chain):
    """
    ä½¿ç”¨"AIé˜…å·ç‰¹çº§æ•™å¸ˆ"æ¨¡å¼ï¼Œå¯¹æ‰€æœ‰é¢˜ç›®è¿›è¡Œæ‰¹é‡ã€æ·±åº¦ã€æ™ºèƒ½çš„åˆ†æå’Œè¯„åˆ†ã€‚
    """
    all_results = []
    prompts_for_ai = []

    # 1. æ”¶é›†æ‰€æœ‰é¢˜ç›®ä¿¡æ¯ï¼Œå¹¶é¢„å…ˆåˆ¤æ–­å®¢è§‚é¢˜å¯¹é”™
    user_answers_dict = {ua['question_id']: ua['student_answer'] for ua in user_answers}
    for q in questions:
        student_ans = user_answers_dict.get(q['id'], "")
        # å¯¹æ‰€æœ‰é¢˜å‹éƒ½å…ˆè¿›è¡Œä¸€æ¬¡åŸºäºè§„åˆ™çš„å¯¹é”™åˆ¤æ–­
        is_correct = (str(student_ans).lower().strip() == str(q['answer']).lower().strip())

        prompts_for_ai.append({
            "question_id": q['id'],
            "question_text": q['question_text'],
            "question_type": q['type'],
            "options": q.get('options', []),
            "standard_answer": q['answer'],
            "student_answer": student_ans,
            "was_correct": is_correct,
            "max_score": q['score']
        })

    # 2. è®¾è®¡ç®€æ´æ˜ç¡®çš„AIæ‰¹æ”¹Prompt
    if prompts_for_ai:
        prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•™å­¦ä¸“å®¶ï¼Œè¯·ä¸ºä»¥ä¸‹{len(prompts_for_ai)}é“é¢˜ç›®è¿›è¡Œè¯¦ç»†çš„æ‰¹æ”¹åˆ†æã€‚

è¦æ±‚ï¼š
1. å¯¹æ¯é“é¢˜ç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œåé¦ˆ
2. åé¦ˆè¦å…·æœ‰æ•™è‚²ä»·å€¼ï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£çŸ¥è¯†ç‚¹
3. å¯¹äºé”™è¯¯ç­”æ¡ˆï¼Œè¦åˆ†æé”™è¯¯åŸå› å¹¶ç»™å‡ºæ­£ç¡®è§£æ
4. å¯¹äºæ­£ç¡®ç­”æ¡ˆï¼Œè¦è¿›è¡ŒçŸ¥è¯†ç‚¹å·©å›ºå’Œæ‰©å±•

è¿”å›JSONæ•°ç»„æ ¼å¼ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
- question_id: é¢˜ç›®ID
- score: å¾—åˆ†ï¼ˆé€‰æ‹©é¢˜ï¼šæ­£ç¡®å¾—æ»¡åˆ†ï¼Œé”™è¯¯å¾—0åˆ†ï¼›ä¸»è§‚é¢˜ï¼šæ ¹æ®è´¨é‡ç»™åˆ†ï¼‰
- feedback: è¯¦ç»†çš„æ•™å­¦åé¦ˆï¼ˆè‡³å°‘100å­—ï¼ŒåŒ…å«åˆ†æã€è§£é‡Šã€å»ºè®®ï¼‰
- knowledge_point: æ ¸å¿ƒçŸ¥è¯†ç‚¹

ç¤ºä¾‹æ ¼å¼ï¼š
[
  {{
    "question_id": 1,
    "score": 5,
    "feedback": "æ™ºèƒ½å¯¼å­¦è¯„è¯­ï¼š1. å­¦ç”Ÿçš„å›ç­”åŸºç¡€ç†è§£æ­£ç¡®ï¼Œæ­£ç¡®ç­”æ¡ˆæ˜¯Aï¼Œå½“å‰é¢˜ç›®åƒä¸ªè®­ç»ƒæ ·æœ¬ä¸­ï¼ŒåŒå­¦å·²ç»å¯¹äº†"æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º"çš„ä¸»è¦æ¦‚å¿µç†è§£ã€‚2. å­¦ç”Ÿé€‰æ‹©çš„è¿™ä¸ªé”™è¯¯é€‰é¡¹åæ˜ å‡ºå¯¹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µç†è§£ä¸å¤Ÿæ·±å…¥ï¼Œå…·ä½“è¡¨ç°åœ¨å¯¹æœºå™¨å­¦ä¹ çš„å®šä¹‰ç†è§£ä¸å¤Ÿå‡†ç¡®ï¼Œè€Œå®é™…å­¦ä¹ è¿‡ç¨‹ä¸­éœ€è¦æ›´æ·±å…¥ç†è§£æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µã€‚3. æ­£ç¡®ç­”æ¡ˆè§£æï¼šæ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼å’Œè¡¨ç¤ºã€‚å»ºè®®å­¦ç”Ÿè¿›ä¸€æ­¥å­¦ä¹ ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†å’Œæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µã€‚",
    "knowledge_point": "æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ"
  }}
]

é¢˜ç›®ä¿¡æ¯ï¼š
{json.dumps(prompts_for_ai, ensure_ascii=False, indent=2)}

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼š
        """
        try:
            response = qa_chain.invoke({"question": prompt})
            result_text = response.get('answer', '').strip()

            # ä½¿ç”¨å¼ºåŒ–ç‰ˆJSONæ•°ç»„è§£æ
            all_results = parse_json_array_robust(result_text, prompts_for_ai)

            if all_results:
                # ä¸ºä¸»è§‚é¢˜åŠ ä¸Šå¯è´¨ç–‘æ ‡è®°ï¼Œå¹¶è¡¥å……max_scoreå­—æ®µ
                for res in all_results:
                    q_info = next(
                        (p for p in prompts_for_ai if p['question_id'] == res['question_id']),
                        None)
                    if q_info:
                        q_type = q_info['question_type']
                        res["allow_dispute"] = (q_type != "multiple_choice")
                        res["max_score"] = q_info['max_score']  # æ·»åŠ max_scoreå­—æ®µ
            else:
                raise ValueError("AIæœªèƒ½è¿”å›æœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼ã€‚")
        except Exception as e:
            # å¦‚æœAIæ‰¹æ”¹å¤±è´¥ï¼Œä¸ºæ‰€æœ‰é¢˜ç›®è¿”å›ä¸€ä¸ªé”™è¯¯ä¿¡æ¯
            print(f"âŒ AIæ‰¹æ”¹å¼‚å¸¸: {str(e)}")
            all_results = create_default_results(prompts_for_ai)

    return sorted(all_results, key=lambda x: x['question_id'])
